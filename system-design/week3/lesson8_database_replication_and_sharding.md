# Week 3, Lesson 8: Database Replication & Sharding

## Table of Contents
- [Why Database Scaling is Different](#why-database-scaling-is-different)
- [Part 1: Database Replication](#part-1-database-replication)
  - [What is Replication?](#what-is-replication)
  - [Why Replicate?](#why-replicate)
- [Replication Strategy 1: Master-Slave (Primary-Replica)](#replication-strategy-1-master-slave-primary-replica)
  - [How It Works](#how-it-works)
  - [The Replication Process](#the-replication-process)
  - [Synchronous vs Asynchronous Replication](#synchronous-vs-asynchronous-replication)
  - [Handling Primary Failure (Failover)](#handling-primary-failure-failover)
  - [Reading from Replicas: Consistency Concerns](#reading-from-replicas-consistency-concerns)
- [Replication Strategy 2: Multi-Master (Active-Active)](#replication-strategy-2-multi-master-active-active)
  - [Why Multi-Master?](#why-multi-master)
  - [The Conflict Problem](#the-conflict-problem)
  - [Conflict Resolution Strategies](#conflict-resolution-strategies)
  - [When to Use Multi-Master](#when-to-use-multi-master)
- [Part 2: Database Sharding](#part-2-database-sharding)
  - [What is Sharding?](#what-is-sharding)
  - [Why Sharding?](#why-sharding)
- [Sharding Strategy 1: Range-Based Sharding](#sharding-strategy-1-range-based-sharding)
- [Sharding Strategy 2: Hash-Based Sharding](#sharding-strategy-2-hash-based-sharding)
- [Sharding Strategy 3: Consistent Hashing](#sharding-strategy-3-consistent-hashing)
- [Sharding Strategy 4: Directory-Based Sharding](#sharding-strategy-4-directory-based-sharding)
- [Sharding Challenges](#sharding-challenges)
  - [Challenge 1: Cross-Shard Queries](#challenge-1-cross-shard-queries)
  - [Challenge 2: Cross-Shard Joins](#challenge-2-cross-shard-joins)
  - [Challenge 3: Cross-Shard Transactions](#challenge-3-cross-shard-transactions)
  - [Challenge 4: Rebalancing](#challenge-4-rebalancing)
  - [Challenge 5: Schema Changes](#challenge-5-schema-changes)
- [Combining Replication and Sharding](#combining-replication-and-sharding)
- [Real-World Examples](#real-world-examples)
- [Decision Framework](#decision-framework)
- [Common Mistakes](#common-mistakes)
- [Key Concepts to Remember](#key-concepts-to-remember)
- [Practice Questions](#practice-questions)
- [Next Up](#next-up)

---

In the previous lesson, we learned about scaling applications horizontally. But what about databases? This lesson covers the two most important techniques for scaling databases: **replication** and **sharding**.

---

## Why Database Scaling is Different

**Applications are (mostly) stateless. Databases are stateful.**

```
Application Scaling (Easy):
[Server 1] â”€â”
[Server 2] â”€â”¼â”€> Any server can handle any request
[Server 3] â”€â”˜    No coordination needed!

Database Scaling (Hard):
[DB 1] â”€â”
[DB 2] â”€â”¼â”€> Data must be consistent!
[DB 3] â”€â”˜    Where does data live?
             What happens on writes?
```

**The Challenge:**
- Data must not be lost
- Data must be consistent
- Queries must find the right data
- Transactions must work correctly

---

## Part 1: Database Replication

### What is Replication?

**Keeping copies of the same data on multiple machines**

```
Primary Database (Source of Truth)
        â”‚
        â”‚ (replication)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â†“               â†“
[Replica 1]  [Replica 2]

All three have the SAME data!
```

### Why Replicate?

**1. High Availability**
```
Primary dies?
        â†“
Replica becomes new primary!
        â†“
Zero data loss, minimal downtime
```

**2. Scale Reads**
```
Before (1 server):
[Primary] â† 10,000 reads/sec ðŸ˜°

After (3 servers):
[Primary]  â† 3,333 reads/sec âœ…
[Replica 1] â† 3,333 reads/sec âœ…
[Replica 2] â† 3,333 reads/sec âœ…
```

**3. Geographic Distribution**
```
US Users â”€â”€â†’ [US Replica]    (20ms latency)
EU Users â”€â”€â†’ [EU Replica]    (20ms latency)
Asia Users â†’ [Asia Replica]  (20ms latency)

vs. Single server in US:
EU Users â”€â”€â†’ [US Server]     (150ms latency!) ðŸ˜°
```

---

## Replication Strategy 1: Master-Slave (Primary-Replica)

### How It Works

```
                    WRITES
                      â”‚
                      â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   PRIMARY    â”‚
              â”‚   (Master)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚    Replication      â”‚
           â†“          â†“          â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Replica â”‚ â”‚ Replica â”‚ â”‚ Replica â”‚
      â”‚    1    â”‚ â”‚    2    â”‚ â”‚    3    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†‘          â†‘          â†‘
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   READS

All writes â†’ Primary
Reads â†’ Any replica (or primary)
```

### The Replication Process

```
Step 1: Client writes to Primary
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INSERT INTO users (name) VALUES     â”‚
â”‚ ('Alice')                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â†“
Step 2: Primary writes to its log
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Binary Log / Write-Ahead Log (WAL)  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ Position 1001: INSERT users Alice   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â†“
Step 3: Replicas pull log and apply
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Replica reads log position 1001     â”‚
â”‚ Applies: INSERT users Alice         â”‚
â”‚ Now replica has same data!          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Synchronous vs Asynchronous Replication

#### Asynchronous Replication (Most Common)

```
Client          Primary         Replica
  â”‚                â”‚               â”‚
  â”‚â”€â”€ INSERT â”€â”€â”€â”€â”€â†’â”‚               â”‚
  â”‚                â”‚â”€â”€ (later) â”€â”€â”€â†’â”‚
  â”‚â†â”€â”€ OK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚               â”‚
  â”‚                â”‚               â”‚

Primary responds BEFORE replica confirms
Fast, but replica might lag behind!
```

**Pros:**
- âœ… Low latency (don't wait for replicas)
- âœ… Primary not affected by slow replicas
- âœ… Works across geographic regions

**Cons:**
- âŒ Replica might have stale data
- âŒ Data loss if primary fails before replication

**Replication Lag:**
```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
Primary:  [Write A] [Write B] [Write C]
Replica:  [Write A] [Write B] . . . waiting . . .
                              â†‘
                         Replication Lag
                         (usually ms to seconds)
```

#### Synchronous Replication

```
Client          Primary         Replica
  â”‚                â”‚               â”‚
  â”‚â”€â”€ INSERT â”€â”€â”€â”€â”€â†’â”‚               â”‚
  â”‚                â”‚â”€â”€ Replicate â”€â†’â”‚
  â”‚                â”‚â†â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚â†â”€â”€ OK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚               â”‚
  â”‚                â”‚               â”‚

Primary waits for replica confirmation
Slower, but guaranteed consistency!
```

**Pros:**
- âœ… No data loss on primary failure
- âœ… Replicas always up-to-date

**Cons:**
- âŒ Higher latency (wait for slowest replica)
- âŒ Primary blocked if replica is slow/down
- âŒ Doesn't work well across regions

#### Semi-Synchronous (Hybrid)

```
Primary waits for AT LEAST ONE replica

[Primary] â”€â”€â†’ [Replica 1] âœ… (fast, nearby)
          â””â”€â†’ [Replica 2] ... (slow, far away)

Primary responds after Replica 1 confirms
Replica 2 catches up asynchronously

Balance of speed and safety!
```

### Handling Primary Failure (Failover)

```
Normal Operation:
[Primary] â†â”€â”€ Writes
    â”‚
    â””â”€â”€â†’ [Replica 1] [Replica 2]

Primary Crashes! ðŸ’¥:
[Primary] âŒ
    â”‚
    â””â”€â”€â†’ [Replica 1] [Replica 2]

Failover:
[Primary] âŒ (removed)

[Replica 1] â†â”€â”€ Promoted to Primary!
    â”‚
    â””â”€â”€â†’ [Replica 2] (now replicates from Replica 1)
```

**Failover Steps:**
1. Detect primary failure (health checks)
2. Choose a replica to promote
3. Promote replica to primary
4. Reconfigure other replicas
5. Update application to use new primary

**Automatic vs Manual Failover:**
```
Manual:
- Operator decides when to failover
- Safer (no false positives)
- Slower (depends on human response)

Automatic:
- System detects and failovers automatically
- Faster recovery
- Risk of false positives (split-brain!)
```

### Reading from Replicas: Consistency Concerns

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’

User writes profile update (Primary):
    "name: Alice â†’ Alicia"
              â”‚
              â†“
User immediately reads profile (Replica):
    "name: Alice"  â† STALE! Replication not complete!

This is "read-your-writes" inconsistency
```

**Solutions:**

**1. Read from Primary after Write**
```javascript
// After updating profile
await db.primary.update(user)

// Read from primary for 5 seconds
if (recentlyUpdated) {
    return db.primary.read(userId)
} else {
    return db.replica.read(userId)
}
```

**2. Sticky Reads**
```
User session always reads from same replica
That replica has all their recent writes
```

**3. Causal Consistency**
```
Track replication position:
- Write returns position: 1001
- Read includes position: "at least 1001"
- Replica waits until caught up to 1001
```

---

## Replication Strategy 2: Multi-Master (Active-Active)

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Master 1   â”‚â†â”€â”€â”€â”€â”€â”€â”€â†’â”‚   Master 2   â”‚
â”‚  (US-East)   â”‚         â”‚  (US-West)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘                        â†‘
       â”‚                        â”‚
    Writes                   Writes
    Reads                    Reads

Both can accept writes!
Both replicate to each other!
```

### Why Multi-Master?

**1. Higher Write Availability**
```
Single Master:
Master down â†’ NO WRITES! âŒ

Multi-Master:
Master 1 down â†’ Write to Master 2 âœ…
```

**2. Geographic Write Performance**
```
Single Master (in US-East):
EU User write â†’ US-East â†’ 150ms latency ðŸ˜°

Multi-Master:
EU User write â†’ EU Master â†’ 20ms latency âœ…
```

### The Conflict Problem

```
Same record, different masters, same time:

Master 1 (US-East):
UPDATE users SET name = 'Alice' WHERE id = 1

Master 2 (US-West):
UPDATE users SET name = 'Alicia' WHERE id = 1

Both succeed locally... but which one wins?

CONFLICT! ðŸ’¥
```

### Conflict Resolution Strategies

**1. Last Write Wins (LWW)**
```
Use timestamps to pick winner:

Master 1: name = 'Alice'  @ 10:00:00.001
Master 2: name = 'Alicia' @ 10:00:00.002  â† WINS!

Simple but dangerous:
- Clock skew can cause wrong winner
- Data can be silently lost
```

**2. Application-Level Resolution**
```
Store both versions:
{
  id: 1,
  name: ['Alice', 'Alicia'],  // Conflict!
  conflict: true
}

Application or user resolves:
- Show user both options
- Merge logic specific to domain
```

**3. Conflict-Free Replicated Data Types (CRDTs)**
```
Special data structures that merge automatically:

Counter CRDT:
Master 1: counter + 5
Master 2: counter + 3
Merged:   counter + 8 (both apply!)

Set CRDT:
Master 1: add 'apple'
Master 2: add 'banana'
Merged:   {'apple', 'banana'}

No conflicts possible by design!
```

**4. Conflict Avoidance**
```
Partition writes by key:

User ID 1-1000:    â†’ Master 1 only
User ID 1001-2000: â†’ Master 2 only

No conflicts because different masters own different data!
(This is essentially sharding)
```

### When to Use Multi-Master

**Good for:**
- âœ… Geo-distributed writes (users in multiple regions)
- âœ… High write availability requirements
- âœ… Systems with mostly non-overlapping writes
- âœ… Offline-capable applications

**Avoid for:**
- âŒ High-conflict workloads
- âŒ Strong consistency requirements
- âŒ Complex transaction requirements
- âŒ Simple applications (overkill)

---

## Part 2: Database Sharding

### What is Sharding?

**Splitting data across multiple databases**

```
Before (Single Database):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         All Users (10M)         â”‚
â”‚  ID 1-10,000,000 in one place!  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Getting slow... ðŸ˜°

After (Sharded):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard 1  â”‚ â”‚  Shard 2  â”‚ â”‚  Shard 3  â”‚
â”‚  Users    â”‚ â”‚  Users    â”‚ â”‚  Users    â”‚
â”‚  1-3.3M   â”‚ â”‚ 3.3M-6.6M â”‚ â”‚ 6.6M-10M  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Each shard handles 1/3 of queries!
```

### Why Sharding?

**1. Scale Beyond Single Machine**
```
Single server limits:
- Storage: 10TB max
- Connections: 10,000 max
- CPU: 128 cores max

Need more? â†’ Shard!
```

**2. Scale Writes**
```
Replication only scales reads!
All writes still go to primary.

Sharding scales BOTH:
Write to Shard 1: User 1 update
Write to Shard 2: User 1M update
Write to Shard 3: User 5M update

Parallel writes to different shards!
```

**3. Reduce Query Scope**
```
Query all 10M users:
â””â”€> Scan 10M rows ðŸ˜°

Query shard with 3M users:
â””â”€> Scan 3M rows âœ…

Each shard is smaller â†’ faster queries
```

---

## Sharding Strategy 1: Range-Based Sharding

### How It Works

```
Divide data by ranges of a key:

User ID 1-1,000,000:      â†’ Shard 1
User ID 1,000,001-2,000,000: â†’ Shard 2
User ID 2,000,001-3,000,000: â†’ Shard 3
```

### Implementation

```
Shard Mapping Table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Range        â”‚  Shard  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1 - 1,000,000   â”‚ Shard 1 â”‚
â”‚ 1M - 2,000,000  â”‚ Shard 2 â”‚
â”‚ 2M - 3,000,000  â”‚ Shard 3 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query routing:
SELECT * FROM users WHERE id = 1,500,000
â†’ Check mapping: 1.5M is in 1M-2M range
â†’ Route to Shard 2
```

### Advantages

```
âœ… Simple to understand and implement
âœ… Range queries are efficient
   "Get users 100-200" â†’ Single shard!
âœ… Easy to add new shards
   "Add Shard 4 for users 3M-4M"
```

### Disadvantages: Hot Spots!

```
Problem: Uneven distribution

New users get sequential IDs:
- User 2,999,001 â†’ Shard 3
- User 2,999,002 â†’ Shard 3
- User 2,999,003 â†’ Shard 3

ALL new users go to Shard 3! ðŸ”¥

Shard 3: 100% of new writes
Shard 1: 0% of new writes (old users)
Shard 2: 0% of new writes (old users)
```

**Solutions:**
- Use random/UUID keys (no sequential hotspots)
- Use hash-based sharding instead
- Regularly rebalance shards

---

## Sharding Strategy 2: Hash-Based Sharding

### How It Works

```
hash(key) % number_of_shards = shard

Example:
hash("user_123") = 7823456
7823456 % 3 = 1
â†’ User goes to Shard 1

hash("user_456") = 9823411
9823411 % 3 = 2
â†’ User goes to Shard 2
```

### Implementation

```python
def get_shard(user_id, num_shards):
    hash_value = hash(user_id)
    shard_num = hash_value % num_shards
    return shards[shard_num]

# Example
user_id = "user_12345"
shard = get_shard(user_id, 3)  # Returns Shard 0, 1, or 2
```

### Advantages

```
âœ… Even distribution
   Hash function spreads data uniformly
   No hot spots!

âœ… Any key type works
   Strings, UUIDs, composite keys
   All get hashed the same way
```

### Disadvantages

```
âŒ Range queries are expensive!
   "Get users 100-200"
   â†’ Could be on ANY shard
   â†’ Must query ALL shards! ðŸ˜°

âŒ Resharding is painful!
   Adding Shard 4 changes the math:

   Before: hash % 3 = shard
   After:  hash % 4 = shard

   Most data needs to move!
```

**The Resharding Problem:**
```
Before (3 shards):
hash("user_1") % 3 = 1 â†’ Shard 1
hash("user_2") % 3 = 2 â†’ Shard 2
hash("user_3") % 3 = 0 â†’ Shard 0

After (4 shards):
hash("user_1") % 4 = 1 â†’ Shard 1 âœ… (same)
hash("user_2") % 4 = 2 â†’ Shard 2 âœ… (same)
hash("user_3") % 4 = 3 â†’ Shard 3 âŒ (MOVED!)

~75% of data moves when adding 1 shard!
```

---

## Sharding Strategy 3: Consistent Hashing

### The Problem with Regular Hashing

```
Add/remove shard = massive data movement

3 â†’ 4 shards: ~75% data moves
4 â†’ 5 shards: ~80% data moves

This is terrible for production systems!
```

### How Consistent Hashing Works

```
Imagine a ring (0 to 2^32):

                    0
                    â”‚
           Shard 1 â—â”‚
                   â•±â”‚â•²
                  â•± â”‚ â•²
         Shard 2â—   â”‚   â—Shard 3
                 â•²  â”‚  â•±
                  â•² â”‚ â•±
                   â•²â”‚â•±
                    â”‚
              2^32 (wraps to 0)

Each shard is placed on the ring
Data is placed on the ring (by hash)
Data goes to the NEXT shard clockwise
```

### Consistent Hashing in Action

```
Placing data:
hash("user_1") = position on ring
Walk clockwise â†’ hit Shard 2
user_1 â†’ Shard 2

hash("user_2") = different position
Walk clockwise â†’ hit Shard 3
user_2 â†’ Shard 3
```

### Adding a Shard (Minimal Disruption!)

```
Before:
        Shard 1
       â•±       â•²
      â•±         â•²
Shard 2â”€â”€â”€â”€â”€â”€â”€â”€â”€Shard 3

user_1 â†’ Shard 2
user_2 â†’ Shard 3
user_3 â†’ Shard 1

After (add Shard 4 between 2 and 3):
        Shard 1
       â•±       â•²
      â•±         â•²
Shard 2â”€â”€Shard 4â”€â”€Shard 3

user_1 â†’ Shard 4 (moved!)
user_2 â†’ Shard 3 (same)
user_3 â†’ Shard 1 (same)

Only ~25% of data moves (1/N where N = new shard count)
```

### Virtual Nodes

```
Problem: Uneven distribution with few shards

Shard 1 â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
        â†‘                       â†‘
   Very far apart = too much data!

Solution: Virtual nodes
Each shard has multiple positions on ring:

Shard 1: V1.1, V1.2, V1.3, V1.4
Shard 2: V2.1, V2.2, V2.3, V2.4
Shard 3: V3.1, V3.2, V3.3, V3.4

      V1.1    V2.2    V3.1
        â—       â—       â—
       â•±                 â•²
   V3.2â—                   â—V1.3
       â•²                 â•±
        â—       â—       â—
      V2.1    V1.2    V3.3

More even distribution!
Better load balancing!
```

---

## Sharding Strategy 4: Directory-Based Sharding

### How It Works

```
Lookup service tells you where data lives:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Shard Directory         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User 1     â†’ Shard A        â”‚
â”‚ User 2     â†’ Shard B        â”‚
â”‚ User 3     â†’ Shard A        â”‚
â”‚ User 1000  â†’ Shard C        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query: "Where is User 2?"
Directory: "Shard B"
â†’ Route query to Shard B
```

### Advantages

```
âœ… Complete flexibility
   Any key can go anywhere

âœ… Easy rebalancing
   Move data, update directory

âœ… Custom placement
   "VIP users on fast shard"
   "Same-company users together"
```

### Disadvantages

```
âŒ Directory is a single point of failure
   Directory down = can't route queries!

âŒ Extra hop for every query
   Query directory first, then shard

âŒ Directory can become bottleneck
   All queries hit directory

Solution: Cache directory heavily!
```

---

## Sharding Challenges

### Challenge 1: Cross-Shard Queries

```
Query: "Find all orders for user 123"

If orders are sharded by order_id:
- User 123's orders could be on ANY shard
- Must query ALL shards!
- Slow and expensive ðŸ˜°

                    [Application]
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                 â†“                 â†“
   [Shard 1]         [Shard 2]         [Shard 3]
   Orders 1-1M       Orders 1M-2M      Orders 2M-3M
   (some user 123)   (some user 123)   (some user 123)
```

**Solution: Co-locate Related Data**
```
Shard by user_id instead:

Shard 1: Users 1-1M     + All their orders
Shard 2: Users 1M-2M    + All their orders
Shard 3: Users 2M-3M    + All their orders

"Find all orders for user 123" â†’ Single shard!
```

### Challenge 2: Cross-Shard Joins

```sql
-- This is hard across shards!
SELECT u.name, o.total
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE o.date > '2024-01-01'

If users and orders are on different shards:
â†’ Can't do a simple JOIN
â†’ Must fetch from both, join in application
```

**Solutions:**
1. **Denormalize:** Store user info in orders table
2. **Co-locate:** Same shard key for related tables
3. **Application-side joins:** Fetch separately, join in code
4. **Avoid joins:** Design for single-shard queries

### Challenge 3: Cross-Shard Transactions

```
Transaction: Transfer $100 from User A to User B

BEGIN TRANSACTION;
UPDATE accounts SET balance = balance - 100 WHERE user = 'A';
UPDATE accounts SET balance = balance + 100 WHERE user = 'B';
COMMIT;

If User A on Shard 1, User B on Shard 2:
â†’ Can't do atomic transaction across shards!
â†’ What if Shard 2 fails after Shard 1 commits?
â†’ Money disappears! ðŸ’¸ðŸ˜±
```

**Solutions:**

**1. Two-Phase Commit (2PC)**
```
Phase 1 - Prepare:
Coordinator: "Can you commit?"
Shard 1: "Yes, prepared"
Shard 2: "Yes, prepared"

Phase 2 - Commit:
Coordinator: "Commit!"
Shard 1: "Committed"
Shard 2: "Committed"

Guarantees atomicity but:
- Slow (multiple round trips)
- Blocking (locks held during prepare)
- Coordinator failure = stuck transactions
```

**2. Saga Pattern**
```
Execute steps with compensating actions:

Step 1: Debit User A (-$100)
Step 2: Credit User B (+$100)

If Step 2 fails:
â†’ Compensate Step 1: Credit User A (+$100)

Eventually consistent, not ACID
But works across shards!
```

**3. Avoid Cross-Shard Transactions**
```
Design so transactions stay on one shard:

Transfer between accounts of SAME user?
â†’ Same shard, normal transaction!

Transfer between DIFFERENT users?
â†’ Use eventual consistency
â†’ Or use a central ledger service
```

### Challenge 4: Rebalancing

```
Over time, shards become uneven:

Shard 1: 5M users (overloaded! ðŸ”¥)
Shard 2: 2M users
Shard 3: 3M users

Need to move data from Shard 1 to others
Without downtime!
```

**Rebalancing Steps:**
```
1. Copy data from Shard 1 to Shard 2
   (Shard 1 still serving traffic)

2. Update routing to point to Shard 2
   (New writes go to Shard 2)

3. Verify Shard 2 has all data

4. Delete data from Shard 1

5. Repeat for other overloaded ranges
```

### Challenge 5: Schema Changes

```
ALTER TABLE across 100 shards?

Bad approach:
for shard in shards:
    shard.execute("ALTER TABLE...")
â†’ Takes hours
â†’ Shards out of sync during migration
â†’ Failures leave inconsistent state

Better approach:
1. Deploy code that handles both schemas
2. Migrate shards one by one
3. Monitor for issues
4. Remove old schema code
```

---

## Combining Replication and Sharding

```
Real production setup:

                [Application]
                      â”‚
              [Router/Proxy]
                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                 â†“                 â†“
[Shard 1]        [Shard 2]         [Shard 3]
    â”‚                 â”‚                 â”‚
â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”
â†“       â†“        â†“       â†“        â†“       â†“
[P][R][R]        [P][R][R]        [P][R][R]

Each shard has:
- 1 Primary (writes)
- 2 Replicas (reads + failover)

Sharding for scale
Replication for availability
```

---

## Real-World Examples

### Example 1: Instagram

```
Challenge: 1 billion users, photos, likes, comments

Solution: Sharding by user_id

Shard assignment:
user_id â†’ shard_id via consistent hashing

Co-located data per shard:
- User profile
- User's photos
- User's followers/following
- User's likes

"Show user's feed" = mostly single shard!
```

### Example 2: Uber

```
Challenge: Real-time location of millions of drivers

Solution: Geographic sharding

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           World Map             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Shard 1â”‚ Shard 2â”‚    Shard 3    â”‚
â”‚  NYC   â”‚   LA   â”‚   Chicago     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Shard 4â”‚ Shard 5â”‚    Shard 6    â”‚
â”‚ Miami  â”‚ Seattleâ”‚   Denver      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

"Find drivers near me" â†’ Single shard!
Riders and drivers in same city â†’ Same shard
```

### Example 3: Discord

```
Challenge: Billions of messages across millions of servers

Solution: Shard by Discord server (guild)

Shard 1: Guild IDs ending in 0
Shard 2: Guild IDs ending in 1
...
Shard 9: Guild IDs ending in 9

All messages for a guild â†’ Same shard
"Load channel history" â†’ Single shard query!

But: Large servers (1M+ members) need special handling
â†’ Further partition by channel
```

---

## Decision Framework

### When to Use Replication Only

```
âœ… Read-heavy workload (90%+ reads)
âœ… Dataset fits on single machine
âœ… Need high availability
âœ… Need geographic read distribution
âœ… Strong consistency requirements

Examples:
- Content management systems
- Product catalogs
- Configuration services
```

### When to Add Sharding

```
âœ… Dataset too large for one machine
âœ… Write-heavy workload
âœ… Single primary can't handle write load
âœ… Need to scale beyond single machine limits

Examples:
- Social media (billions of posts)
- E-commerce (millions of orders)
- IoT (billions of sensor readings)
```

### Comparison

| Aspect | Replication Only | Sharding |
|--------|------------------|----------|
| **Read Scale** | Excellent | Excellent |
| **Write Scale** | Limited | Excellent |
| **Data Size** | Limited | Unlimited |
| **Complexity** | Low | High |
| **Consistency** | Easy | Challenging |
| **Transactions** | Normal | Complex |
| **Query Flexibility** | Full | Limited |

---

## Common Mistakes

### Mistake 1: Sharding Too Early
```
âŒ "We might have 1M users someday, let's shard now!"

Start simple:
1. Single database
2. Add read replicas
3. Optimize queries
4. Vertical scale database
5. Shard only when necessary

Sharding adds massive complexity!
```

### Mistake 2: Wrong Shard Key
```
âŒ Shard by created_date
   â†’ All new data hits same shard!

âŒ Shard by random field
   â†’ Related data scattered everywhere!

âœ… Shard by access pattern
   â†’ Data accessed together stays together
```

### Mistake 3: Ignoring Cross-Shard Operations
```
âŒ "We'll figure out joins later"
   â†’ Design forces expensive cross-shard queries

âœ… Design schema around shard boundaries
   â†’ Most queries hit single shard
```

### Mistake 4: No Rebalancing Plan
```
âŒ "Shards will stay balanced"
   â†’ 1 year later: Shard 1 = 80%, Shard 2 = 10%, Shard 3 = 10%

âœ… Monitor shard sizes
   â†’ Automate or plan for rebalancing
```

---

## Key Concepts to Remember

1. **Replication** = Same data on multiple machines (availability + read scale)
2. **Sharding** = Different data on different machines (write scale + capacity)
3. **Master-Slave** = Single writer, multiple readers
4. **Multi-Master** = Multiple writers (beware conflicts!)
5. **Range sharding** = Simple but can have hot spots
6. **Hash sharding** = Even distribution but hard to rebalance
7. **Consistent hashing** = Minimal data movement on changes
8. **Co-locate related data** on same shard to avoid cross-shard queries
9. **Cross-shard transactions** are hard - design to avoid them
10. **Start simple** - replication before sharding!

---

## Practice Questions

**Q1:** You have a database with 80% reads and 20% writes. Currently using a single PostgreSQL server that's reaching capacity. What's your first step? Justify your answer.

**Q2:** An e-commerce platform shards their orders table by order_id using hash sharding. Users complain that "View My Orders" is slow. Why? How would you fix it?

**Q3:** You're designing a chat application like Slack. Messages need to be queried by:
- Channel (most common)
- User (less common)
- Date range (analytics)

What's your sharding strategy? What trade-offs are you making?

**Q4:** Your multi-master MySQL setup has conflicts occurring 100 times per day. 90% are on the "page_views" counter column. How do you eliminate these conflicts?

**Q5:** Explain why consistent hashing is better than regular hash sharding when you need to add or remove shards frequently.

**Q6:** Design the replication and sharding strategy for a social media app with:
- 100M users
- Users post ~2 times per day
- Users read feeds ~50 times per day
- Users are globally distributed

**Q7:** A team wants to shard by user_id but needs to run this query efficiently:
```sql
SELECT COUNT(*) FROM orders
WHERE created_at > '2024-01-01'
GROUP BY product_id
```
What's the problem? Propose solutions.

---

## Next Up

In Lesson 9, we'll explore **Stateless vs Stateful Architecture** - the key to making your applications truly scalable!
